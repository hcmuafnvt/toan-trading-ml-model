#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Stage 4.4 â€” Feature Refinement & Quick Retrain (GBPUSD)

Purpose:
- Align tsfresh feature windows with labels from Stage 3.
- Enrich with directional meta-features (mean/std/slope/etc.).
- Scale features and run a quick LightGBM sanity test to check alpha.

Inputs:
    logs/stage4_tsfresh_features_gbpusd.csv
    data/stage3_train_ready.parquet
Outputs:
    logs/stage4_refined_features_gbpusd.csv
    logs/stage4_refined_model_gbpusd.txt
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timezone
from sklearn.preprocessing import StandardScaler
import lightgbm as lgb
from sklearn.metrics import roc_auc_score, accuracy_score, classification_report


FEATURE_CSV = "logs/stage4_tsfresh_features_gbpusd.csv"
LABEL_FILE  = "data/stage3_train_ready.parquet"
OUT_FEATURE = "logs/stage4_refined_features_gbpusd.csv"
OUT_MODEL   = "logs/stage4_refined_model_gbpusd.txt"


def log(msg: str):
    now = datetime.now(timezone.utc).strftime("[%Y-%m-%d %H:%M:%S UTC]")
    print(f"{now} {msg}", flush=True)


def main():
    log("ðŸš€ Stage 4.4 â€” Feature Refinement & Quick Retrain (GBPUSD)")

    # -----------------------------
    # 1ï¸âƒ£ Load data
    # -----------------------------
    features = pd.read_csv(FEATURE_CSV, index_col=0, parse_dates=True)
    log(f"ðŸ“¥ Loaded features: {features.shape}")

    labels = pd.read_parquet(LABEL_FILE)
    labels.index = pd.to_datetime(labels.index, utc=True)

    # Standardize label column names
    if "target_label" not in labels.columns:
        labels = labels.rename(columns={
            "lbl_mc_012": "target_label",
            "mask_train": "target_is_trainable",
            "reason": "target_drop_reason"
        })

    # Keep only needed columns (but don't filter yet)
    labels = labels[["target_label", "target_is_trainable"]].copy()

    # Sort for merge_asof
    features = features.sort_index()
    labels = labels.sort_index()

    # -----------------------------
    # 2ï¸âƒ£ Align features â†” labels
    # -----------------------------
    log("ðŸ”— Aligning features with labels via merge_asof...")
    # âœ… merge_asof: features â†’ labels (forward)
    merged = pd.merge_asof(
        features.sort_index(),
        labels.sort_index(),
        left_index=True,
        right_index=True,
        direction="forward",            # láº¥y label ngay sau window_end_time
        tolerance=pd.Timedelta("48h"),  # cho phÃ©p lá»‡ch tá»‘i Ä‘a 2 ngÃ y
    )

    log(f"[DEBUG] merged shape pre-dropna: {merged.shape}")
    log(f"[DEBUG] merged time range: {merged.index.min()} â†’ {merged.index.max()}")
    log(f"[DEBUG] NaN ratio: {merged.isna().mean().mean():.3f}")    

    # Only now filter trainable
    if "target_is_trainable" in merged.columns:
        merged = merged[merged["target_is_trainable"] == 1]
        log(f"[DEBUG] kept trainable rows: {len(merged)}")

    if merged.empty:
        raise RuntimeError("âŒ Merged dataset empty after alignment or filtering â€” check label timestamps.")

    X = merged.drop(columns=["target_label", "target_is_trainable"])
    y = merged["target_label"].astype(int)
    y = y.clip(upper=1)  # 0=SHORT, 1=LONG (gom class 2 vÃ o 1)

    log(f"ðŸ”— Final aligned samples: {X.shape}")

    # -----------------------------
    # 3ï¸âƒ£ Directional meta-features
    # -----------------------------
    log("âž• Adding directional meta-features...")
    X["feat_mean"] = X.mean(axis=1)
    X["feat_std"] = X.std(axis=1)
    X["feat_skew"] = X.skew(axis=1)
    X["feat_kurt"] = X.kurtosis(axis=1)
    X["feat_slope"] = X["feat_mean"].diff().fillna(0)
    X["feat_volratio"] = (X["feat_std"] / (X["feat_mean"].abs() + 1e-6)).fillna(0)

    # -----------------------------
    # 4ï¸âƒ£ Standardization
    # -----------------------------
    log("âš–ï¸ Scaling features (z-score)...")
    scaler = StandardScaler()
    X_scaled = pd.DataFrame(
        scaler.fit_transform(X),
        index=X.index,
        columns=X.columns
    )
    
    # âœ… Fill NaN sau scale (LightGBM khÃ´ng nháº­n NaN)
    X_scaled = X_scaled.fillna(0)

    Path("logs").mkdir(exist_ok=True)
    X_scaled.to_csv(OUT_FEATURE)
    log(f"ðŸ’¾ Saved refined features â†’ {OUT_FEATURE} ({X_scaled.shape})")

    # -----------------------------
    # 5ï¸âƒ£ Quick LightGBM train/test
    # -----------------------------
    log("âš™ï¸ Quick train/validation split...")
    split = int(len(X_scaled) * 0.8)
    X_train, X_valid = X_scaled.iloc[:split], X_scaled.iloc[split:]
    y_train, y_valid = y.iloc[:split], y.iloc[split:]

    params = dict(
        objective="binary",
        metric=["binary_logloss", "auc"],
        learning_rate=0.05,
        num_leaves=64,
        min_data_in_leaf=50,
        feature_fraction=0.8,
        bagging_fraction=0.8,
        bagging_freq=5,
        seed=42,
        n_jobs=28,
    )

    dtrain = lgb.Dataset(X_train, label=y_train)
    dvalid = lgb.Dataset(X_valid, label=y_valid)

    log("ðŸš€ Training LightGBM...")
    model = lgb.train(
        params,
        dtrain,
        valid_sets=[dtrain, dvalid],
        num_boost_round=500,
        callbacks=[lgb.early_stopping(30), lgb.log_evaluation(50)],
    )

    # -----------------------------
    # 6ï¸âƒ£ Evaluation
    # -----------------------------
    preds = model.predict(X_valid)
    auc = roc_auc_score(y_valid, preds)
    preds_bin = (preds > 0.5).astype(int)
    acc = accuracy_score(y_valid, preds_bin)

    log(f"ðŸ“Š Validation AUC: {auc:.4f}")
    log(f"ðŸ“Š Accuracy: {acc:.4f}")
    log("ðŸ”Ž Classification report:")
    print(classification_report(y_valid, preds_bin, target_names=["SHORT(0)", "LONG(1)"]))

    model.save_model(OUT_MODEL)
    log(f"ðŸ’¾ Model saved â†’ {OUT_MODEL}")
    log("âœ… Stage 4.4 completed successfully")


if __name__ == "__main__":
    main()